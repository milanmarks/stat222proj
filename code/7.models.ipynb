{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import classification_report, average_precision_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "\n",
    "from pyod.models.xgbod import XGBOD\n",
    "from joblib import dump, load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/train_test_set.csv')\n",
    "data = pd.get_dummies(data, columns = ['main_industry', 'year'])\n",
    "\n",
    "X_train = data[data['train'] == 1].drop(columns=['cik', 'sic', 'sic_description', 'filedate', 'accession_num',\n",
    "       'primary_doc', 'filelink', 'bank_status', 'train'])\n",
    "X_test = data[data['train'] == 0].drop(columns=['cik', 'sic', 'sic_description', 'filedate', 'accession_num',\n",
    "       'primary_doc', 'filelink', 'bank_status', 'train'])\n",
    "y_train = data[data['train']==1]['bank_status']\n",
    "y_test = data[data['train']==0]['bank_status']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standardization for numerical features \n",
    "scaler = StandardScaler().fit(X_train.iloc[:, 0:23])\n",
    "scaled_terms_train = pd.DataFrame(scaler.transform(X_train.iloc[:, 0:23]), columns = X_train.columns[0:23])\n",
    "X_num_train = pd.concat([scaled_terms_train, X_train.iloc[:,28:].reset_index(drop = True)], axis = 1)\n",
    "scaled_terms_test = pd.DataFrame(scaler.transform(X_test.iloc[:, 0:23]), columns = X_test.columns[0:23])\n",
    "X_num_test = pd.concat([scaled_terms_test, X_test.iloc[:,28:].reset_index(drop = True)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../res/lsa.npy\", \"rb\") as f:\n",
    "    X_lsa_train = np.load(f)\n",
    "    X_lsa_test = np.load(f)\n",
    "\n",
    "with open(\"../res/bert.npy\", \"rb\") as f:\n",
    "    X_bert_train = np.load(f)\n",
    "    X_bert_test = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sent_train = X_train.sentiment\n",
    "X_sent_test = X_test.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numsenti_train = np.hstack([X_num_train.to_numpy(), X_sent_train.to_numpy().reshape((-1, 1))])\n",
    "X_numsenti_test = np.hstack([X_num_test.to_numpy(), X_sent_test.to_numpy().reshape((-1, 1))])\n",
    "X_numlsa_train = np.hstack([X_num_train.to_numpy(), X_lsa_train])\n",
    "X_numlsa_test = np.hstack([X_num_test.to_numpy(), X_lsa_test])\n",
    "X_numbert_train = np.hstack([X_num_train.to_numpy(), X_bert_train])\n",
    "X_numbert_test = np.hstack([X_num_test.to_numpy(), X_bert_test])\n",
    "X_all_train = np.hstack([X_num_train.to_numpy(), X_lsa_train, X_bert_train, X_sent_train.to_numpy().reshape((-1, 1))])\n",
    "X_all_test = np.hstack([X_num_test.to_numpy(), X_lsa_test, X_bert_test, X_sent_test.to_numpy().reshape((-1, 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure(clf, X_train, y_train, X_test, y_test):\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    y_pred_prob_test = clf.predict_proba(X_test)[:,1]\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    y_pred_prob_train = clf.predict_proba(X_train)[:,1]\n",
    "\n",
    "    print(classification_report(y_train, y_pred_train))\n",
    "    print(\"Train average precision:\", average_precision_score(y_train, y_pred_prob_train))\n",
    "\n",
    "    print(classification_report(y_test, y_pred_test))\n",
    "    print(\"Test average precision:\", average_precision_score(y_test, y_pred_prob_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_xgbod(X_train, y_train, X_test, y_test, load_path=None, save_path=None):\n",
    "    if load_path:\n",
    "        clf = load(load_path)\n",
    "    else:\n",
    "        clf = XGBOD(random_state=42)\n",
    "        clf.fit(X_train, y_train)\n",
    "    y_train_scores = clf.decision_scores_\n",
    "    y_test_scores = clf.decision_function(X_test)\n",
    "    if save_path:\n",
    "        dump(clf, save_path)\n",
    "    print(f\"PR AUC on train set:{average_precision_score(y_train, y_train_scores)}\")\n",
    "    print(f\"PR AUC on test set:{average_precision_score(y_test, y_test_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_all(X_train, y_train, X_test, y_test, load_path=None, save_path=None):\n",
    "    print(\"=\"*20+\"logistic regression\"+\"=\"*20)\n",
    "    lr = LogisticRegression(\n",
    "        max_iter = int(1e6),\n",
    "        tol = 1e-6, \n",
    "        fit_intercept = False, \n",
    "        class_weight='balanced', \n",
    "        random_state=42,\n",
    "        )\n",
    "    lr.fit(X_train, y_train)\n",
    "    measure(lr, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    print(\"=\"*20+\"SVM\"+\"=\"*20)\n",
    "    svm = SVC(class_weight = 'balanced', random_state = 42, probability=True, kernel=\"rbf\", gamma=\"scale\")\n",
    "    svm.fit(X_train, y_train)\n",
    "    measure(svm, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    print(\"=\"*20+\"Decision Tree\"+\"=\"*20)\n",
    "    tree = DecisionTreeClassifier(class_weight = 'balanced', random_state = 42)\n",
    "    tree.fit(X_train, y_train)\n",
    "    measure(tree, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    print(\"=\"*20+\"Easy Ensemble\"+\"=\"*20)\n",
    "    eec = EasyEnsembleClassifier(random_state=42, estimator=RandomForestClassifier(), n_estimators=20)\n",
    "    eec.fit(X_train, y_train)\n",
    "    measure(eec, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    print(\"=\"*20+\"XGBOD\"+\"=\"*20)\n",
    "    if load_path:\n",
    "        clf = load(load_path)\n",
    "    else:\n",
    "        clf = XGBOD(random_state=42)\n",
    "        clf.fit(X_train, y_train)\n",
    "    y_train_scores = clf.decision_scores_\n",
    "    y_test_scores = clf.decision_function(X_test)\n",
    "    if save_path:\n",
    "        dump(clf, save_path)\n",
    "    print(f\"PR AUC on train set:{average_precision_score(y_train, y_train_scores)}\")\n",
    "    print(f\"PR AUC on test set:{average_precision_score(y_test, y_test_scores)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "running all baseline models: random pick, logistic regression, svm, decision tree, easyensemble, xgbod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================logistic regression====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92      7025\n",
      "           1       0.03      0.95      0.07        40\n",
      "\n",
      "    accuracy                           0.85      7065\n",
      "   macro avg       0.52      0.90      0.49      7065\n",
      "weighted avg       0.99      0.85      0.91      7065\n",
      "\n",
      "Train average precision: 0.051595569012316364\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92      2330\n",
      "           1       0.05      0.69      0.09        26\n",
      "\n",
      "    accuracy                           0.85      2356\n",
      "   macro avg       0.52      0.77      0.50      2356\n",
      "weighted avg       0.99      0.85      0.91      2356\n",
      "\n",
      "Test average precision: 0.06921478857138398\n",
      "====================SVM====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95      7025\n",
      "           1       0.06      1.00      0.11        40\n",
      "\n",
      "    accuracy                           0.91      7065\n",
      "   macro avg       0.53      0.95      0.53      7065\n",
      "weighted avg       0.99      0.91      0.95      7065\n",
      "\n",
      "Train average precision: 0.1421055403437637\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95      2330\n",
      "           1       0.09      0.77      0.16        26\n",
      "\n",
      "    accuracy                           0.91      2356\n",
      "   macro avg       0.54      0.84      0.55      2356\n",
      "weighted avg       0.99      0.91      0.94      2356\n",
      "\n",
      "Test average precision: 0.16388198832184972\n",
      "====================Decision Tree====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7025\n",
      "           1       0.93      1.00      0.96        40\n",
      "\n",
      "    accuracy                           1.00      7065\n",
      "   macro avg       0.97      1.00      0.98      7065\n",
      "weighted avg       1.00      1.00      1.00      7065\n",
      "\n",
      "Train average precision: 0.9947674418604651\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      2330\n",
      "           1       0.27      0.15      0.20        26\n",
      "\n",
      "    accuracy                           0.99      2356\n",
      "   macro avg       0.63      0.57      0.59      2356\n",
      "weighted avg       0.98      0.99      0.98      2356\n",
      "\n",
      "Test average precision: 0.05329390473702868\n",
      "====================Easy Ensemble====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.82      0.90      7025\n",
      "           1       0.03      1.00      0.06        40\n",
      "\n",
      "    accuracy                           0.82      7065\n",
      "   macro avg       0.52      0.91      0.48      7065\n",
      "weighted avg       0.99      0.82      0.90      7065\n",
      "\n",
      "Train average precision: 0.7948702412558267\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.82      0.90      2330\n",
      "           1       0.05      0.85      0.09        26\n",
      "\n",
      "    accuracy                           0.82      2356\n",
      "   macro avg       0.52      0.83      0.50      2356\n",
      "weighted avg       0.99      0.82      0.89      2356\n",
      "\n",
      "Test average precision: 0.4239947505685783\n",
      "====================XGBOD====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haodong/opt/anaconda3/lib/python3.9/site-packages/pyod/models/base.py:430: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "/Users/haodong/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [15:24:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR AUC on train set:0.989292786502539\n",
      "PR AUC on test set:0.37057125187594436\n"
     ]
    }
   ],
   "source": [
    "fit_all(X_num_train, y_train, X_num_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================logistic regression====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92      7025\n",
      "           1       0.03      0.95      0.07        40\n",
      "\n",
      "    accuracy                           0.85      7065\n",
      "   macro avg       0.52      0.90      0.49      7065\n",
      "weighted avg       0.99      0.85      0.91      7065\n",
      "\n",
      "Train average precision: 0.05100537089528237\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92      2330\n",
      "           1       0.05      0.73      0.10        26\n",
      "\n",
      "    accuracy                           0.85      2356\n",
      "   macro avg       0.52      0.79      0.51      2356\n",
      "weighted avg       0.99      0.85      0.91      2356\n",
      "\n",
      "Test average precision: 0.0700993973977713\n",
      "====================SVM====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95      7025\n",
      "           1       0.06      1.00      0.11        40\n",
      "\n",
      "    accuracy                           0.91      7065\n",
      "   macro avg       0.53      0.95      0.53      7065\n",
      "weighted avg       0.99      0.91      0.95      7065\n",
      "\n",
      "Train average precision: 0.14330651304782402\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95      2330\n",
      "           1       0.08      0.73      0.15        26\n",
      "\n",
      "    accuracy                           0.91      2356\n",
      "   macro avg       0.54      0.82      0.55      2356\n",
      "weighted avg       0.99      0.91      0.94      2356\n",
      "\n",
      "Test average precision: 0.14863115430767732\n",
      "====================Decision Tree====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7025\n",
      "           1       0.93      1.00      0.96        40\n",
      "\n",
      "    accuracy                           1.00      7065\n",
      "   macro avg       0.97      1.00      0.98      7065\n",
      "weighted avg       1.00      1.00      1.00      7065\n",
      "\n",
      "Train average precision: 0.9947674418604651\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      2330\n",
      "           1       0.25      0.15      0.19        26\n",
      "\n",
      "    accuracy                           0.99      2356\n",
      "   macro avg       0.62      0.57      0.59      2356\n",
      "weighted avg       0.98      0.99      0.98      2356\n",
      "\n",
      "Test average precision: 0.05036350180662574\n",
      "====================Easy Ensemble====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.82      0.90      7025\n",
      "           1       0.03      1.00      0.06        40\n",
      "\n",
      "    accuracy                           0.82      7065\n",
      "   macro avg       0.52      0.91      0.48      7065\n",
      "weighted avg       0.99      0.82      0.90      7065\n",
      "\n",
      "Train average precision: 0.8024226817858485\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.82      0.90      2330\n",
      "           1       0.05      0.88      0.10        26\n",
      "\n",
      "    accuracy                           0.82      2356\n",
      "   macro avg       0.52      0.85      0.50      2356\n",
      "weighted avg       0.99      0.82      0.89      2356\n",
      "\n",
      "Test average precision: 0.41228167148525846\n",
      "====================XGBOD====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haodong/opt/anaconda3/lib/python3.9/site-packages/pyod/models/base.py:430: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "/Users/haodong/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [15:26:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR AUC on train set:0.9836518533616058\n",
      "PR AUC on test set:0.38430904813049777\n"
     ]
    }
   ],
   "source": [
    "fit_all(X_numsenti_train, y_train, X_numsenti_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================logistic regression====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97      7025\n",
      "           1       0.09      1.00      0.17        40\n",
      "\n",
      "    accuracy                           0.95      7065\n",
      "   macro avg       0.55      0.97      0.57      7065\n",
      "weighted avg       0.99      0.95      0.97      7065\n",
      "\n",
      "Train average precision: 0.2870463483727455\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97      2330\n",
      "           1       0.10      0.58      0.17        26\n",
      "\n",
      "    accuracy                           0.94      2356\n",
      "   macro avg       0.55      0.76      0.57      2356\n",
      "weighted avg       0.99      0.94      0.96      2356\n",
      "\n",
      "Test average precision: 0.19737365729485093\n",
      "====================SVM====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98      7025\n",
      "           1       0.11      1.00      0.20        40\n",
      "\n",
      "    accuracy                           0.95      7065\n",
      "   macro avg       0.56      0.98      0.59      7065\n",
      "weighted avg       0.99      0.95      0.97      7065\n",
      "\n",
      "Train average precision: 0.4015983392286616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98      2330\n",
      "           1       0.17      0.73      0.27        26\n",
      "\n",
      "    accuracy                           0.96      2356\n",
      "   macro avg       0.58      0.84      0.62      2356\n",
      "weighted avg       0.99      0.96      0.97      2356\n",
      "\n",
      "Test average precision: 0.22861221573538484\n",
      "====================Decision Tree====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7025\n",
      "           1       0.93      1.00      0.96        40\n",
      "\n",
      "    accuracy                           1.00      7065\n",
      "   macro avg       0.97      1.00      0.98      7065\n",
      "weighted avg       1.00      1.00      1.00      7065\n",
      "\n",
      "Train average precision: 0.9947674418604651\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      2330\n",
      "           1       0.15      0.12      0.13        26\n",
      "\n",
      "    accuracy                           0.98      2356\n",
      "   macro avg       0.57      0.55      0.56      2356\n",
      "weighted avg       0.98      0.98      0.98      2356\n",
      "\n",
      "Test average precision: 0.027980932480083584\n",
      "====================Easy Ensemble====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95      7025\n",
      "           1       0.06      1.00      0.11        40\n",
      "\n",
      "    accuracy                           0.91      7065\n",
      "   macro avg       0.53      0.95      0.53      7065\n",
      "weighted avg       0.99      0.91      0.95      7065\n",
      "\n",
      "Train average precision: 0.9610006139347967\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92      2330\n",
      "           1       0.07      0.96      0.13        26\n",
      "\n",
      "    accuracy                           0.85      2356\n",
      "   macro avg       0.53      0.91      0.52      2356\n",
      "weighted avg       0.99      0.85      0.91      2356\n",
      "\n",
      "Test average precision: 0.32743865875528005\n",
      "====================XGBOD====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haodong/opt/anaconda3/lib/python3.9/site-packages/pyod/models/base.py:430: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "/Users/haodong/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [15:29:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR AUC on train set:0.9963952761173408\n",
      "PR AUC on test set:0.5983131426579215\n"
     ]
    }
   ],
   "source": [
    "fit_all(X_numlsa_train, y_train, X_numlsa_test, y_test, save_path=\"../res/numlsa.joblib\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================logistic regression====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7025\n",
      "           1       0.82      1.00      0.90        40\n",
      "\n",
      "    accuracy                           1.00      7065\n",
      "   macro avg       0.91      1.00      0.95      7065\n",
      "weighted avg       1.00      1.00      1.00      7065\n",
      "\n",
      "Train average precision: 0.9815198768603048\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      2330\n",
      "           1       0.62      0.31      0.41        26\n",
      "\n",
      "    accuracy                           0.99      2356\n",
      "   macro avg       0.80      0.65      0.70      2356\n",
      "weighted avg       0.99      0.99      0.99      2356\n",
      "\n",
      "Test average precision: 0.23819223918358684\n",
      "====================SVM====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7025\n",
      "           1       0.75      1.00      0.86        40\n",
      "\n",
      "    accuracy                           1.00      7065\n",
      "   macro avg       0.88      1.00      0.93      7065\n",
      "weighted avg       1.00      1.00      1.00      7065\n",
      "\n",
      "Train average precision: 0.8818267183958557\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      2330\n",
      "           1       0.50      0.31      0.38        26\n",
      "\n",
      "    accuracy                           0.99      2356\n",
      "   macro avg       0.75      0.65      0.69      2356\n",
      "weighted avg       0.99      0.99      0.99      2356\n",
      "\n",
      "Test average precision: 0.2319314921482431\n",
      "====================Decision Tree====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7025\n",
      "           1       0.93      1.00      0.96        40\n",
      "\n",
      "    accuracy                           1.00      7065\n",
      "   macro avg       0.97      1.00      0.98      7065\n",
      "weighted avg       1.00      1.00      1.00      7065\n",
      "\n",
      "Train average precision: 0.9947674418604651\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      2330\n",
      "           1       0.56      0.19      0.29        26\n",
      "\n",
      "    accuracy                           0.99      2356\n",
      "   macro avg       0.77      0.60      0.64      2356\n",
      "weighted avg       0.99      0.99      0.99      2356\n",
      "\n",
      "Test average precision: 0.11575101940127408\n",
      "====================Easy Ensemble====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      7025\n",
      "           1       0.33      1.00      0.50        40\n",
      "\n",
      "    accuracy                           0.99      7065\n",
      "   macro avg       0.67      0.99      0.75      7065\n",
      "weighted avg       1.00      0.99      0.99      7065\n",
      "\n",
      "Train average precision: 0.7702897101404684\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      2330\n",
      "           1       0.21      0.35      0.26        26\n",
      "\n",
      "    accuracy                           0.98      2356\n",
      "   macro avg       0.60      0.67      0.63      2356\n",
      "weighted avg       0.98      0.98      0.98      2356\n",
      "\n",
      "Test average precision: 0.271282366319764\n",
      "====================XGBOD====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haodong/opt/anaconda3/lib/python3.9/site-packages/pyod/models/base.py:430: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "/Users/haodong/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [15:32:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR AUC on train set:0.9963952761173407\n",
      "PR AUC on test set:0.22297754715783152\n"
     ]
    }
   ],
   "source": [
    "fit_all(X_numbert_train, y_train, X_numbert_test, y_test, save_path=\"../res/numbert.joblib\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================logistic regression====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7025\n",
      "           1       0.83      1.00      0.91        40\n",
      "\n",
      "    accuracy                           1.00      7065\n",
      "   macro avg       0.92      1.00      0.95      7065\n",
      "weighted avg       1.00      1.00      1.00      7065\n",
      "\n",
      "Train average precision: 0.9851680046720357\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      2330\n",
      "           1       0.62      0.31      0.41        26\n",
      "\n",
      "    accuracy                           0.99      2356\n",
      "   macro avg       0.80      0.65      0.70      2356\n",
      "weighted avg       0.99      0.99      0.99      2356\n",
      "\n",
      "Test average precision: 0.23010375779410494\n",
      "====================SVM====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7025\n",
      "           1       0.75      1.00      0.86        40\n",
      "\n",
      "    accuracy                           1.00      7065\n",
      "   macro avg       0.88      1.00      0.93      7065\n",
      "weighted avg       1.00      1.00      1.00      7065\n",
      "\n",
      "Train average precision: 0.8861495726779041\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      2330\n",
      "           1       0.50      0.31      0.38        26\n",
      "\n",
      "    accuracy                           0.99      2356\n",
      "   macro avg       0.75      0.65      0.69      2356\n",
      "weighted avg       0.99      0.99      0.99      2356\n",
      "\n",
      "Test average precision: 0.23255868709687863\n",
      "====================Decision Tree====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7025\n",
      "           1       0.93      1.00      0.96        40\n",
      "\n",
      "    accuracy                           1.00      7065\n",
      "   macro avg       0.97      1.00      0.98      7065\n",
      "weighted avg       1.00      1.00      1.00      7065\n",
      "\n",
      "Train average precision: 0.9947674418604651\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      2330\n",
      "           1       0.54      0.27      0.36        26\n",
      "\n",
      "    accuracy                           0.99      2356\n",
      "   macro avg       0.77      0.63      0.68      2356\n",
      "weighted avg       0.99      0.99      0.99      2356\n",
      "\n",
      "Test average precision: 0.15303493033021567\n",
      "====================Easy Ensemble====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      7025\n",
      "           1       0.33      1.00      0.50        40\n",
      "\n",
      "    accuracy                           0.99      7065\n",
      "   macro avg       0.67      0.99      0.75      7065\n",
      "weighted avg       1.00      0.99      0.99      7065\n",
      "\n",
      "Train average precision: 0.8310878964511265\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      2330\n",
      "           1       0.21      0.35      0.26        26\n",
      "\n",
      "    accuracy                           0.98      2356\n",
      "   macro avg       0.60      0.67      0.63      2356\n",
      "weighted avg       0.98      0.98      0.98      2356\n",
      "\n",
      "Test average precision: 0.27010081063829183\n",
      "====================XGBOD====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haodong/opt/anaconda3/lib/python3.9/site-packages/pyod/models/base.py:430: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "/Users/haodong/opt/anaconda3/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [15:36:43] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR AUC on train set:0.9963952761173408\n",
      "PR AUC on test set:0.20294233915279603\n"
     ]
    }
   ],
   "source": [
    "fit_all(X_all_train, y_train, X_all_test, y_test, save_path=\"../res/all.joblib\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
